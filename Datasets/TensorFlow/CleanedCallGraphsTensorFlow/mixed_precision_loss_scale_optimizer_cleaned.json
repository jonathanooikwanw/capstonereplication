{"mixed_precision.loss_scale_optimizer": ["experimental.mixed_precision.register_loss_scale_wrapper"], "mixed_precision.loss_scale_optimizer._UnwrapPreventer.__init__": [], "mixed_precision.loss_scale_optimizer._is_all_finite": ["ops.math_ops.reduce_all", "ops.math_ops.is_finite"], "ops.math_ops.is_finite": [], "ops.math_ops.reduce_all": [], "mixed_precision.loss_scale_optimizer._op_in_graph_mode": ["eager.context.executing_eagerly"], "eager.context.executing_eagerly": [], "mixed_precision.loss_scale_optimizer._assign_if_finite": ["ops.control_flow_ops.cond", "ops.math_ops.is_finite"], "mixed_precision.loss_scale_optimizer._assign_if_finite.<lambda1>": ["ops.math_ops.is_finite"], "ops.control_flow_ops.cond": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.__init__": ["<builtin>.float", "ops.math_ops.is_finite", "<builtin>.super", "<builtin>.int"], "<builtin>.super": [], "<builtin>.float": [], "<builtin>.int": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState._add_weight": ["ops.variable_scope.variable", "base.Trackable._handle_deferred_dependencies", "keras.backend.track_variable", "eager.context.executing_eagerly", "framework.ops.get_default_graph"], "ops.variable_scope.variable": [], "framework.ops.get_default_graph": [], "base.Trackable._handle_deferred_dependencies": [], "keras.backend.track_variable": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState._checkpoint_dependencies": ["eager.context.executing_eagerly", "framework.ops.get_default_graph", "tracking.base.TrackableReference", "<builtin>.super", "<builtin>.sorted"], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState._checkpoint_dependencies.<lambda1>": [], "<builtin>.sorted": [], "tracking.base.TrackableReference": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState._lookup_dependency": ["eager.context.executing_eagerly", "framework.ops.get_default_graph", "<builtin>.super"], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.initial_loss_scale": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.growth_steps": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.multiplier": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.current_loss_scale": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.counter": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.__call__": ["framework.ops.convert_to_tensor_v2_with_dispatch"], "framework.ops.convert_to_tensor_v2_with_dispatch": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.update": ["distribute.distribution_strategy_context.in_cross_replica_context", "ops.control_flow_ops.cond", "ops.control_flow_ops.cond", "distribute.distribution_strategy_context.has_strategy", "distribute.distribution_strategy_context.get_strategy", "util.nest.flatten"], "util.nest.flatten": [], "distribute.distribution_strategy_context.has_strategy": [], "distribute.distribution_strategy_context.in_cross_replica_context": [], "distribute.distribution_strategy_context.get_strategy": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.update.update_if_finite_grads": ["ops.control_flow_ops.cond"], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.update.update_if_finite_grads.incr_loss_scale": ["ops.control_flow_ops.cond", "ops.control_flow_ops.group"], "ops.control_flow_ops.group": [], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.update.update_if_finite_grads.<lambda1>": ["ops.control_flow_ops.group"], "mixed_precision.loss_scale_optimizer._DynamicLossScaleState.update.update_if_not_finite_grads": ["ops.control_flow_ops.group", "ops.math_ops.maximum"], "ops.math_ops.maximum": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.__init__": ["base_delegate.DelegatingTrackableMixin._track_trackable", "<builtin>.TypeError", "<builtin>.getattr", "base_delegate.DelegatingTrackableMixin._track_trackable", "base_delegate.DelegatingTrackableMixin.__init__", "base_delegate.DelegatingTrackableMixin.__init__", "<builtin>.float", "<builtin>.isinstance", "<builtin>.ValueError", "base_delegate.DelegatingTrackableMixin.__init__"], "<builtin>.isinstance": [], "<builtin>.TypeError": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer._raise_if_strategy_unsupported": ["distribute.distribution_strategy_context.get_strategy", "distribute.distribution_strategy_context.get_strategy", "<builtin>.ValueError", "<builtin>.isinstance"], "<builtin>.getattr": [], "<builtin>.ValueError": [], "base_delegate.DelegatingTrackableMixin.__init__": [], "base_delegate.DelegatingTrackableMixin._track_trackable": [], "mixed_precision.loss_scale_optimizer.FakeOptimizerForRestoration.__init__": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.dynamic": ["<builtin>.isinstance"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.loss_scale": ["<builtin>.isinstance", "framework.ops.convert_to_tensor_v2_with_dispatch"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.dynamic_counter": ["<builtin>.isinstance"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.initial_scale": ["<builtin>.isinstance"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.dynamic_growth_steps": ["<builtin>.isinstance"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.inner_optimizer": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.get_scaled_loss": ["<builtin>.callable", "ops.math_ops.cast"], "<builtin>.callable": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.get_scaled_loss.new_loss": ["ops.math_ops.cast"], "ops.math_ops.cast": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.get_unscaled_gradients": ["ops.math_ops.cast"], "mixed_precision.loss_scale_optimizer._multiply_gradient": ["ops.math_ops.cast", "framework.ops.IndexedSlices", "<builtin>.isinstance"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer._compute_gradients": ["<builtin>.list", "eager.backprop.GradientTape", "<builtin>.zip", "eager.backprop.GradientTape", "eager.backprop.GradientTape"], "eager.backprop.GradientTape": [], "<builtin>.zip": [], "<builtin>.list": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.get_gradients": ["eager.backprop.GradientTape", "eager.backprop.GradientTape"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer._create_all_weights": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.apply_gradients": ["distribute.distribution_strategy_context.in_cross_replica_context", "optimizer_v2.utils.strategy_supports_no_merge_call", "ops.control_flow_ops.group", "ops.control_flow_ops.group", "framework.smart_cond.smart_cond", "framework.smart_cond.smart_cond", "distribute.distribution_strategy_context.get_replica_context", "optimizer_v2.utils.filter_empty_gradients", "<builtin>.tuple", "<builtin>.ValueError", "optimizer_v2.utils.filter_empty_gradients"], "optimizer_v2.utils.filter_empty_gradients": [], "<builtin>.tuple": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.apply_gradients.do_not_apply_fn": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.apply_gradients._if_should_apply_grads": ["ops.control_flow_ops.no_op", "<builtin>.isinstance", "ops.control_flow_ops.no_op"], "ops.control_flow_ops.no_op": [], "optimizer_v2.utils.strategy_supports_no_merge_call": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.apply_gradients.apply_fn": ["ops.control_flow_ops.no_op"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer._apply_gradients": ["<builtin>.zip", "<builtin>.list"], "framework.smart_cond.smart_cond": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.apply_gradients._apply_gradients_cross_replica": ["framework.smart_cond.smart_cond", "framework.smart_cond.smart_cond", "ops.control_flow_ops.group"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.apply_gradients._apply_gradients_cross_replica.apply_fn": [], "distribute.distribution_strategy_context.get_replica_context": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.get_config": ["keras.optimizers.serialize"], "keras.optimizers.serialize": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.from_config": ["mixed_precision.loss_scale.deserialize", "mixed_precision.loss_scale.deserialize", "<builtin>.isinstance", "<builtin>.ValueError", "keras.optimizers.deserialize"], "mixed_precision.loss_scale.deserialize": [], "keras.optimizers.deserialize": [], "mixed_precision.loss_scale_optimizer.strategy_supports_loss_scaling": ["distribute.distribution_strategy_context.get_strategy", "<builtin>.isinstance", "distribute.distribution_strategy_context.has_strategy"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.iterations": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.get_slot_names": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.variables": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.weights": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.get_weights": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.set_weights": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.clipnorm": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.global_clipnorm": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.clipvalue": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer._aggregate_gradients": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer._restore_slot_variable": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer._create_or_restore_slot_variable": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.get_slot": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.add_slot": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.__getattribute__": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.__dir__": ["<builtin>.list", "<builtin>.set", "<builtin>.super"], "<builtin>.set": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.__setattr__": ["<builtin>.super"], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.learning_rate": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizer.lr": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizerV1.__init__": ["<builtin>.TypeError", "mixed_precision.loss_scale.deserialize", "<builtin>.super", "platform.tf_logging.warning", "<builtin>.isinstance", "<builtin>.ValueError"], "platform.tf_logging.warning": [], "mixed_precision.loss_scale_optimizer.LossScaleOptimizerV1.from_config": ["experimental.loss_scale.FixedLossScale", "mixed_precision.loss_scale.deserialize", "experimental.loss_scale.DynamicLossScale", "experimental.loss_scale.DynamicLossScale", "<builtin>.isinstance", "<builtin>.ValueError", "keras.optimizers.deserialize"], "experimental.loss_scale.DynamicLossScale": [], "experimental.loss_scale.FixedLossScale": [], "mixed_precision.loss_scale_optimizer.FakeOptimizerForRestoration.get_slot_names": [], "mixed_precision.loss_scale_optimizer.FakeOptimizerForRestoration._create_or_restore_slot_variable": [], "experimental.mixed_precision.register_loss_scale_wrapper": [], "framework.ops.IndexedSlices": []}
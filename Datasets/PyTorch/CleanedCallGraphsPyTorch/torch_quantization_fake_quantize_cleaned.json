{"fake_quantize": [], "fake_quantize._is_per_channel": [], "fake_quantize._is_per_tensor": [], "fake_quantize._is_symmetric_quant": [], "fake_quantize.FakeQuantizeBase.__init__": ["<builtin>.super", "torch.tensor", "abc.ABC.register_buffer"], "<builtin>.super": [], "torch.tensor": [], "abc.ABC.register_buffer": [], "fake_quantize.FakeQuantizeBase": ["abc.abstractmethod", "<builtin>.classmethod", "torch.jit.export"], "abc.abstractmethod": [], "fake_quantize.FakeQuantizeBase.forward": [], "fake_quantize.FakeQuantizeBase.calculate_qparams": [], "torch.jit.export": [], "fake_quantize.FakeQuantizeBase.enable_fake_quant": [], "fake_quantize.FakeQuantizeBase.disable_fake_quant": ["fake_quantize.FakeQuantizeBase.enable_fake_quant"], "fake_quantize.FakeQuantizeBase.enable_observer": [], "fake_quantize.FakeQuantizeBase.disable_observer": ["fake_quantize.FakeQuantizeBase.enable_observer"], "<builtin>.classmethod": [], "fake_quantize.FakeQuantize.__init__": ["<builtin>.super", "observer.MovingAverageMinMaxObserver", "<builtin>.hasattr", "fake_quantize._is_per_tensor", "torch.iinfo", "<builtin>.str", "fake_quantize._is_per_channel", "torch.tensor", "abc.ABC.register_buffer"], "observer.MovingAverageMinMaxObserver": [], "torch.iinfo": [], "<builtin>.hasattr": [], "<builtin>.str": [], "fake_quantize.FakeQuantize": ["torch.jit.export"], "fake_quantize.FakeQuantize.calculate_qparams": [], "fake_quantize.FakeQuantize.forward": ["torch.fake_quantize_per_tensor_affine", "abc.ABC.zero_point.copy_", "fake_quantize.FakeQuantize.calculate_qparams", "abc.ABC.scale.copy_", "abc.ABC.scale.resize_", "abc.ABC.zero_point.resize_", "torch.fake_quantize_per_channel_affine"], "abc.ABC.scale.resize_": [], "abc.ABC.zero_point.resize_": [], "abc.ABC.scale.copy_": [], "abc.ABC.zero_point.copy_": [], "torch.fake_quantize_per_channel_affine": [], "torch.fake_quantize_per_tensor_affine": [], "fake_quantize.FakeQuantize.extra_repr": [], "fake_quantize.FakeQuantize._save_to_state_dict": ["<builtin>.super"], "fake_quantize.FakeQuantize._load_from_state_dict": ["<builtin>.super", "abc.ABC.zero_point.copy_", "abc.ABC.scale.copy_", "abc.ABC.scale.resize_", "abc.ABC.zero_point.resize_", "torch.jit.is_scripting"], "torch.jit.is_scripting": [], "fake_quantize.FixedQParamsFakeQuantize.__init__": ["<builtin>.super", "fake_quantize._is_per_tensor", "<builtin>.str", "torch.tensor", "abc.ABC.register_buffer"], "fake_quantize.FixedQParamsFakeQuantize.forward": ["torch.fake_quantize_per_tensor_affine"], "fake_quantize.FixedQParamsFakeQuantize": ["torch.jit.export"], "fake_quantize.FixedQParamsFakeQuantize.calculate_qparams": [], "fake_quantize.FixedQParamsFakeQuantize.extra_repr": [], "fake_quantize.FusedMovingAvgObsFakeQuantize.__init__": ["<builtin>.super", "fake_quantize._is_symmetric_quant", "<builtin>.isinstance", "torch.tensor", "abc.ABC.register_buffer"], "<builtin>.isinstance": [], "fake_quantize.FusedMovingAvgObsFakeQuantize": ["torch.jit.export"], "fake_quantize.FusedMovingAvgObsFakeQuantize.calculate_qparams": [], "fake_quantize.FusedMovingAvgObsFakeQuantize.extra_repr": [], "fake_quantize.FusedMovingAvgObsFakeQuantize.forward": ["torch.fused_moving_avg_obs_fake_quant"], "torch.fused_moving_avg_obs_fake_quant": [], "fake_quantize._is_fake_quant_script_module": ["<builtin>.isinstance", "re.sub"], "re.sub": [], "fake_quantize.disable_fake_quant": ["<builtin>.isinstance", "fake_quantize._is_fake_quant_script_module"], "fake_quantize.enable_fake_quant": ["<builtin>.isinstance", "fake_quantize._is_fake_quant_script_module"], "fake_quantize.disable_observer": ["<builtin>.isinstance", "fake_quantize._is_fake_quant_script_module"], "fake_quantize.enable_observer": ["<builtin>.isinstance", "fake_quantize._is_fake_quant_script_module"]}
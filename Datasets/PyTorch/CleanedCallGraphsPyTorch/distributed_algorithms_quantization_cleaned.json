{"quantization": ["torch.finfo"], "torch.finfo": [], "quantization.DQuantType.__str__": [], "quantization._fp32_to_fp16_with_clamp": ["torch.clamp"], "torch.clamp": [], "quantization._quantize_tensor": ["<builtin>.type", "quantization._fp32_to_fp16_with_clamp", "<builtin>.isinstance", "<builtin>.RuntimeError"], "<builtin>.isinstance": [], "<builtin>.type": [], "<builtin>.RuntimeError": [], "quantization._quantize_tensor_list": ["<builtin>.type", "<builtin>.all", "<builtin>.isinstance", "<builtin>.RuntimeError", "quantization._quantize_tensor"], "<builtin>.all": [], "quantization._dequantize_tensor": ["<builtin>.type", "<builtin>.isinstance", "<builtin>.RuntimeError"], "quantization._dequantize_tensor_list": ["<builtin>.type", "<builtin>.all", "quantization._dequantize_tensor", "<builtin>.isinstance", "<builtin>.RuntimeError"], "quantization.auto_quantize": ["functools.wraps"], "functools.wraps": [], "quantization.auto_quantize.wrapper": ["torch.distributed.all_to_all", "<builtin>.RuntimeError", "quantization._dequantize_tensor_list", "quantization._quantize_tensor", "quantization._quantize_tensor_list", "<builtin>.enumerate", "torch.distributed.all_gather"], "torch.distributed.all_gather": [], "<builtin>.enumerate": [], "torch.distributed.all_to_all": []}
{"activation": [], "activation.MultiheadAttention.__init__": ["torch.quantization.DeQuantStub", "torch.nn.quantized.FloatFunctional", "torch.quantization.QuantStub", "torch.nn.Linear", "<builtin>.super"], "<builtin>.super": [], "torch.nn.Linear": [], "torch.nn.quantized.FloatFunctional": [], "torch.quantization.QuantStub": [], "torch.quantization.DeQuantStub": [], "activation.MultiheadAttention._get_name": [], "activation.MultiheadAttention.from_float": ["<builtin>.type", "activation.MultiheadAttention.__init__", "torch.quantization.prepare", "<builtin>.hasattr", "torch.nn.Parameter"], "<builtin>.type": [], "<builtin>.hasattr": [], "torch.nn.Parameter": [], "torch.quantization.prepare": [], "activation.MultiheadAttention": ["torch.jit.unused"], "torch.jit.unused": [], "activation.MultiheadAttention.dequantize": ["<builtin>.all", "torch.nn.MultiheadAttention.bias_k.dequantize", "torch.nn.MultiheadAttention.bias_v.dequantize", "torch.nn.MultiheadAttention", "torch.nn.Parameter"], "torch.nn.MultiheadAttention": [], "torch.nn.MultiheadAttention.bias_k.dequantize": [], "torch.nn.MultiheadAttention.bias_v.dequantize": [], "<builtin>.all": [], "activation.MultiheadAttention.from_observed": ["torch.quantize_per_tensor", "torch.quantization.convert", "<builtin>.setattr", "torch._choose_qparams_per_tensor"], "torch.quantization.convert": [], "torch._choose_qparams_per_tensor": [], "torch.quantize_per_tensor": [], "<builtin>.setattr": [], "activation.MultiheadAttention.forward": ["activation.MultiheadAttention._forward_impl"], "activation.MultiheadAttention._forward_impl": ["<builtin>.RuntimeError", "torch.nn.MultiheadAttention.bias_k.repeat", "torch.nn.functional.dropout", "<builtin>.list", "torch.nn.MultiheadAttention.bias_v.repeat", "torch.nn.functional.pad", "warnings.warn", "torch.cat", "torch.zeros", "torch.nn.functional.softmax", "torch.bmm", "torch.quantize_per_tensor", "<builtin>.float"], "<builtin>.float": [], "warnings.warn": [], "<builtin>.list": [], "<builtin>.RuntimeError": [], "torch.nn.MultiheadAttention.bias_k.repeat": [], "torch.cat": [], "torch.nn.MultiheadAttention.bias_v.repeat": [], "torch.nn.functional.pad": [], "torch.zeros": [], "torch.bmm": [], "torch.nn.functional.softmax": [], "torch.nn.functional.dropout": []}
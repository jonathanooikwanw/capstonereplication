{"observer": ["<builtin>.str", "abc.ABCMeta"], "observer._PartialWrapper.__init__": [], "observer._PartialWrapper.__call__": [], "observer._PartialWrapper.__repr__": [], "observer._PartialWrapper.with_args": ["observer._with_args"], "observer._with_args": ["observer._PartialWrapper.__init__", "functools.partial"], "observer._PartialWrapper.with_callable_args": [], "functools.partial": [], "observer._with_callable_args": ["observer._PartialWrapper.with_callable_args", "observer._PartialWrapper.__init__", "functools.partial"], "<builtin>.str": [], "abc.ABCMeta": [], "observer.ObserverBase.__init__": ["<builtin>.super"], "<builtin>.super": [], "observer.ObserverBase": ["abc.abstractmethod", "<builtin>.classmethod"], "abc.abstractmethod": [], "observer.ObserverBase.forward": [], "observer.ObserverBase.calculate_qparams": [], "<builtin>.classmethod": [], "observer._ObserverBase.__init__": ["torch.nn.factory_kwargs", "warnings.warn", "utils.calculate_qmin_qmax", "<builtin>.super", "observer._ObserverBase._validate_qmin_qmax", "torch.finfo", "torch.tensor", "torch.nn.Module.register_buffer"], "torch.nn.factory_kwargs": [], "warnings.warn": [], "torch.finfo": [], "torch.tensor": [], "torch.nn.Module.register_buffer": [], "observer._ObserverBase._validate_qmin_qmax": [], "utils.calculate_qmin_qmax": [], "observer._ObserverBase._load_from_state_dict": ["torch.tensor", "<builtin>.super", "torch.finfo"], "observer._ObserverBase": ["torch.jit.export"], "torch.jit.export": [], "observer._ObserverBase._calculate_qparams": ["utils.check_min_max_valid", "torch.zeros", "<builtin>.float", "torch.round", "torch.ones", "<builtin>.len", "torch.clamp", "torch.tensor", "torch.where", "torch.min", "torch.nn.Module.quant_min.size", "torch.nn.Module.quant_min.new_full", "<builtin>.int", "torch.max", "torch.ones_like", "torch.zeros_like"], "utils.check_min_max_valid": [], "torch.zeros_like": [], "torch.min": [], "torch.max": [], "torch.ones": [], "torch.zeros": [], "<builtin>.float": [], "torch.nn.Module.quant_min.size": [], "torch.nn.Module.quant_min.new_full": [], "torch.ones_like": [], "torch.where": [], "torch.round": [], "torch.clamp": [], "<builtin>.len": [], "<builtin>.int": [], "observer._ObserverBase.reset_min_max_vals": ["<builtin>.NotImplementedError"], "<builtin>.NotImplementedError": [], "observer.MinMaxObserver.__init__": ["torch.nn.factory_kwargs", "<builtin>.float", "<builtin>.super", "<builtin>.NotImplementedError", "torch.tensor", "torch.nn.Module.register_buffer"], "observer.MinMaxObserver.forward": ["torch.max", "torch.min", "torch._aminmax"], "torch._aminmax": [], "observer.MinMaxObserver": ["torch.jit.export"], "observer.MinMaxObserver.calculate_qparams": ["observer._ObserverBase._calculate_qparams"], "observer.MinMaxObserver.extra_repr": [], "observer.MinMaxObserver.reset_min_max_vals": ["torch.tensor", "<builtin>.float"], "observer.MovingAverageMinMaxObserver.__init__": ["<builtin>.super"], "observer.MovingAverageMinMaxObserver.forward": ["<builtin>.float", "torch._aminmax"], "observer.PerChannelMinMaxObserver.__init__": ["torch.nn.factory_kwargs", "<builtin>.super", "<builtin>.NotImplementedError", "torch.tensor", "torch.nn.Module.register_buffer"], "observer.PerChannelMinMaxObserver.forward": ["observer.PerChannelMinMaxObserver._forward"], "observer.PerChannelMinMaxObserver._forward": ["<builtin>.range", "<builtin>.len", "torch._aminmax", "torch.min", "torch.max", "torch.flatten"], "<builtin>.range": [], "torch.flatten": [], "observer.PerChannelMinMaxObserver": ["torch.jit.export"], "observer.PerChannelMinMaxObserver.calculate_qparams": ["observer._ObserverBase._calculate_qparams"], "observer.PerChannelMinMaxObserver.extra_repr": [], "observer.PerChannelMinMaxObserver._load_from_state_dict": ["<builtin>.super", "warnings.warn", "torch.jit.is_scripting"], "torch.jit.is_scripting": [], "observer.PerChannelMinMaxObserver._load_from_state_dict_script": ["observer.PerChannelMinMaxObserver._load_from_state_dict"], "observer.PerChannelMinMaxObserver.reset_min_max_vals": ["torch.tensor"], "observer.MovingAveragePerChannelMinMaxObserver.__init__": ["<builtin>.super"], "observer.MovingAveragePerChannelMinMaxObserver.forward": ["<builtin>.len", "<builtin>.range", "torch.flatten", "torch._aminmax"], "observer.HistogramObserver.__init__": ["torch.nn.factory_kwargs", "torch.iinfo", "torch.zeros", "<builtin>.float", "<builtin>.super", "torch.tensor", "torch.nn.Module.register_buffer"], "torch.iinfo": [], "observer.HistogramObserver._get_norm": [], "observer.HistogramObserver._compute_quantization_error": ["torch.zeros", "torch.arange", "torch.ones", "torch.clamp", "torch.tensor", "observer.HistogramObserver._get_norm"], "torch.arange": [], "observer.HistogramObserver._non_linear_param_search": ["<builtin>.float", "torch.sum", "torch.nn.Module.histogram.size", "observer.HistogramObserver._compute_quantization_error", "torch.cumsum"], "torch.nn.Module.histogram.size": [], "torch.sum": [], "torch.cumsum": [], "observer.HistogramObserver._adjust_min_max": ["torch.ceil", "torch.round", "<builtin>.int"], "torch.ceil": [], "observer.HistogramObserver._combine_histograms": ["torch.zeros", "torch.cumsum", "torch.nn.Module.histogram.repeat_interleave"], "torch.nn.Module.histogram.repeat_interleave": [], "observer.HistogramObserver.forward": ["observer.HistogramObserver._combine_histograms", "<builtin>.float", "observer.HistogramObserver._adjust_min_max", "torch.nn.Module.histogram.copy_", "torch._aminmax", "torch.nn.Module.histogram.detach_", "torch.min", "<builtin>.int", "torch.max", "torch.histc"], "torch.histc": [], "torch.nn.Module.histogram.detach_": [], "torch.nn.Module.histogram.copy_": [], "observer.HistogramObserver": ["torch.jit.export"], "observer.HistogramObserver.calculate_qparams": ["<builtin>.float", "warnings.warn", "<builtin>.len", "observer._ObserverBase._calculate_qparams", "torch.tensor", "observer.HistogramObserver._non_linear_param_search"], "observer.HistogramObserver._save_to_state_dict": ["<builtin>.super"], "observer.HistogramObserver._load_from_state_dict": ["<builtin>.float", "<builtin>.setattr", "torch.Size", "<builtin>.super", "torch.tensor"], "torch.Size": [], "<builtin>.setattr": [], "observer.PlaceholderObserver.__init__": ["<builtin>.super"], "observer.PlaceholderObserver.forward": [], "observer.PlaceholderObserver": ["torch.jit.export"], "observer.PlaceholderObserver.calculate_qparams": ["<builtin>.Exception"], "<builtin>.Exception": [], "observer.RecordingObserver.__init__": ["<builtin>.super"], "observer.RecordingObserver.forward": [], "observer.RecordingObserver": ["torch.jit.export"], "observer.RecordingObserver.calculate_qparams": ["<builtin>.Exception"], "observer.RecordingObserver.get_tensor_value": [], "observer.NoopObserver.__init__": ["<builtin>.super"], "observer.NoopObserver.forward": [], "observer.NoopObserver": ["torch.jit.export"], "observer.NoopObserver.calculate_qparams": ["<builtin>.Exception"], "observer._is_observer_script_module": ["re.sub", "<builtin>.isinstance"], "<builtin>.isinstance": [], "re.sub": [], "observer._is_activation_post_process": ["observer._is_observer_script_module", "<builtin>.isinstance"], "observer._is_per_channel_script_obs_instance": ["observer._is_observer_script_module", "<builtin>.isinstance"], "observer.get_observer_state_dict": ["<builtin>.isinstance", "collections.OrderedDict"], "collections.OrderedDict": [], "observer.load_observer_state_dict": ["observer._is_activation_post_process", "observer._is_per_channel_script_obs_instance", "<builtin>.Exception"]}
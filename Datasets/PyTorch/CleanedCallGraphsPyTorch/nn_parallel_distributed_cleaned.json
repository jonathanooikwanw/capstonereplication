{"distributed": ["torch.distributed.is_available", "torch.distributed.rpc.is_available"], "torch.distributed.is_available": [], "torch.distributed.rpc.is_available": [], "distributed._tree_flatten_with_rref": ["torch.utils._pytree.tree_flatten", "<builtin>.isinstance"], "<builtin>.isinstance": [], "torch.utils._pytree.tree_flatten": [], "distributed._tree_unflatten_with_rref": ["torch.utils._pytree.tree_unflatten", "torch.distributed.rpc.RRef"], "torch.utils._pytree.tree_unflatten": [], "torch.distributed.rpc.RRef": [], "distributed._find_tensors": ["distributed._find_tensors", "itertools.chain", "<builtin>.isinstance", "<builtin>.map"], "<builtin>.map": [], "itertools.chain": [], "distributed._dump_DDP_relevant_env_vars": ["<builtin>.print"], "<builtin>.print": [], "distributed._DDPSink.forward": [], "distributed._DDPSink.backward": ["torch.autograd.Variable._execution_engine.queue_callback"], "torch.autograd.Variable._execution_engine.queue_callback": [], "distributed._DDPJoinHook.__init__": ["<builtin>.isinstance", "<builtin>.super"], "<builtin>.super": [], "distributed._DDPJoinHook.main_hook": ["distributed.DistributedDataParallel._match_all_reduce_for_bwd_pass", "distributed.DistributedDataParallel._check_and_sync_module_buffers", "distributed.DistributedDataParallel._match_unused_params_allreduce", "distributed.DistributedDataParallel._check_global_requires_backward_grad_sync"], "distributed.DistributedDataParallel._check_and_sync_module_buffers": ["distributed.DistributedDataParallel.will_sync_module_buffers", "distributed.DistributedDataParallel._find_common_rank", "distributed.DistributedDataParallel._distributed_broadcast_coalesced"], "distributed.DistributedDataParallel._check_global_requires_backward_grad_sync": ["torch.ones", "torch.distributed.all_reduce", "torch.zeros"], "distributed.DistributedDataParallel._match_all_reduce_for_bwd_pass": [], "distributed.DistributedDataParallel._match_unused_params_allreduce": [], "distributed._DDPJoinHook.post_hook": ["distributed.DistributedDataParallel._sync_final_model"], "distributed.DistributedDataParallel._sync_final_model": ["distributed.DistributedDataParallel._find_common_rank", "distributed.DistributedDataParallel._sync_params_and_buffers"], "distributed.DistributedDataParallel.__init__": ["torch.distributed.algorithms.join.Joinable.__init__", "warnings.warn", "<builtin>.list", "distributed.DistributedDataParallel._build_params_for_reducer", "torch.distributed.distributed_c10d._get_default_group", "<builtin>.any", "torch.distributed._verify_model_across_ranks", "<builtin>.isinstance", "torch.distributed._get_debug_mode", "os.environ.get", "<builtin>.len", "<builtin>.hasattr", "distributed.DistributedDataParallel._ddp_init_helper", "distributed.DistributedDataParallel._log_and_throw", "<builtin>.super", "distributed.DistributedDataParallel._build_param_to_name_mapping", "<builtin>.int", "torch._utils._get_device_index", "distributed.DistributedDataParallel._sync_params_and_buffers"], "torch.distributed.algorithms.join.Joinable.__init__": [], "<builtin>.any": [], "distributed.DistributedDataParallel._log_and_throw": ["<builtin>.str"], "<builtin>.len": [], "<builtin>.list": [], "torch._utils._get_device_index": [], "torch.distributed.distributed_c10d._get_default_group": [], "<builtin>.hasattr": [], "warnings.warn": [], "<builtin>.int": [], "os.environ.get": [], "distributed.DistributedDataParallel._build_params_for_reducer": ["distributed.DistributedDataParallel._build_params_for_reducer.produces_sparse_gradient", "distributed.DistributedDataParallel._get_parameters", "<builtin>.set", "<builtin>.list"], "torch.distributed._verify_model_across_ranks": [], "distributed.DistributedDataParallel._sync_params_and_buffers": ["distributed.DistributedDataParallel._distributed_broadcast_coalesced", "<builtin>.len"], "torch.distributed._get_debug_mode": [], "distributed.DistributedDataParallel._build_param_to_name_mapping": ["<builtin>.range", "<builtin>.set", "distributed.DistributedDataParallel._log_and_throw", "<builtin>.len"], "distributed.DistributedDataParallel._ddp_init_helper": ["<builtin>.reversed", "distributed.DistributedDataParallel._passing_sync_batchnorm_handle", "torch.distributed.Reducer", "<builtin>.list", "torch.distributed._compute_bucket_assignment_by_size", "torch.distributed.Logger"], "distributed.DistributedDataParallel._distributed_broadcast_coalesced": ["torch.distributed._broadcast_coalesced"], "<builtin>.str": [], "torch.distributed._compute_bucket_assignment_by_size": [], "<builtin>.reversed": [], "torch.distributed.Reducer": [], "torch.distributed.Logger": [], "distributed.DistributedDataParallel._passing_sync_batchnorm_handle": ["<builtin>.isinstance", "distributed.DistributedDataParallel._log_and_throw"], "distributed.DistributedDataParallel.__getstate__": ["copy.copy", "distributed.DistributedDataParallel._check_default_group"], "distributed.DistributedDataParallel._check_default_group": ["distributed.DistributedDataParallel._log_and_throw", "torch.distributed.distributed_c10d._get_default_group"], "copy.copy": [], "distributed.DistributedDataParallel.__setstate__": ["torch.distributed._get_debug_mode", "distributed.DistributedDataParallel._build_params_for_reducer", "distributed.DistributedDataParallel._build_param_to_name_mapping", "distributed.DistributedDataParallel._set_static_graph", "torch.distributed.distributed_c10d._get_default_group", "distributed.DistributedDataParallel._ddp_init_helper", "modules.Module.__dict__.setdefault", "<builtin>.super"], "modules.Module.__dict__.setdefault": [], "distributed.DistributedDataParallel._set_static_graph": ["warnings.warn"], "<builtin>.set": [], "distributed.DistributedDataParallel._build_params_for_reducer.produces_sparse_gradient": ["<builtin>.isinstance"], "distributed.DistributedDataParallel._get_parameters": ["distributed.DistributedDataParallel._get_parameters.model_parameters"], "<builtin>.range": [], "distributed.DistributedDataParallel._get_parameters.model_parameters": ["<builtin>.hasattr"], "distributed.DistributedDataParallel": ["contextlib.contextmanager"], "contextlib.contextmanager": [], "distributed.DistributedDataParallel.no_sync": [], "distributed.DistributedDataParallel.forward": ["torch.is_grad_enabled", "<builtin>.range", "distributed._tree_unflatten_with_rref", "distributed.DistributedDataParallel._sync_params", "distributed.DistributedDataParallel.to_kwargs", "distributed._tree_flatten_with_rref", "distributed._find_tensors", "<builtin>.list", "<builtin>.len", "torch.autograd.profiler.record_function", "torch.distributed.algorithms.join.Join.notify_join_context", "logging.info", "distributed.DistributedDataParallel._check_global_requires_backward_grad_sync", "<builtin>.enumerate", "torch.autograd.Function.apply", "torch.is_tensor"], "torch.autograd.profiler.record_function": [], "torch.is_grad_enabled": [], "torch.distributed.algorithms.join.Join.notify_join_context": [], "logging.info": [], "distributed.DistributedDataParallel._sync_params": ["distributed.DistributedDataParallel.will_sync_module_buffers", "torch.no_grad", "distributed.DistributedDataParallel._find_common_rank", "distributed.DistributedDataParallel._distributed_broadcast_coalesced"], "distributed.DistributedDataParallel.to_kwargs": ["<builtin>.tuple", "<builtin>.range", "distributed.DistributedDataParallel._recursive_to", "<builtin>.len"], "<builtin>.enumerate": [], "torch.is_tensor": [], "torch.autograd.Function.apply": [], "distributed.DistributedDataParallel.scatter": ["scatter_gather.scatter_kwargs"], "scatter_gather.scatter_kwargs": [], "distributed.DistributedDataParallel._recursive_to": ["distributed.DistributedDataParallel._recursive_to.to_map"], "distributed.DistributedDataParallel._recursive_to.to_map": ["_functions._get_stream", "<builtin>.zip", "<builtin>.len", "<builtin>.list", "torch.cuda.device", "scatter_gather.is_namedtuple", "torch.cuda.current_stream", "<builtin>.type", "torch.device", "<builtin>.isinstance", "torch.cuda.stream", "<builtin>.map"], "torch.device": [], "_functions._get_stream": [], "torch.cuda.stream": [], "torch.cuda.device": [], "torch.cuda.current_stream": [], "scatter_gather.is_namedtuple": [], "<builtin>.type": [], "<builtin>.zip": [], "<builtin>.tuple": [], "distributed.DistributedDataParallel.gather": ["distributed.DistributedDataParallel.gather"], "distributed.DistributedDataParallel.train": ["<builtin>.super"], "distributed.DistributedDataParallel._schedule_shadow_all_reduce_for_fwd_pass": ["torch.distributed.all_reduce", "torch.zeros"], "torch.zeros": [], "torch.distributed.all_reduce": [], "torch.ones": [], "distributed.DistributedDataParallel.will_sync_module_buffers": ["<builtin>.len"], "distributed.DistributedDataParallel._find_common_rank": ["torch.tensor", "torch.distributed.all_reduce", "distributed.DistributedDataParallel._log_and_throw"], "distributed.DistributedDataParallel.join": ["torch.distributed.algorithms.join.Join"], "torch.distributed.algorithms.join.Join": [], "distributed.DistributedDataParallel.join_hook": ["distributed._DDPJoinHook.__init__"], "distributed.DistributedDataParallel.join_device": [], "distributed.DistributedDataParallel.join_process_group": [], "distributed.DistributedDataParallel.register_comm_hook": ["torch.distributed._register_comm_hook", "distributed.DistributedDataParallel._check_comm_hook"], "distributed.DistributedDataParallel._check_comm_hook": ["inspect.signature", "distributed.DistributedDataParallel._log_and_throw", "<builtin>.callable"], "torch.distributed._register_comm_hook": [], "distributed.DistributedDataParallel._register_builtin_comm_hook": ["<builtin>.str", "torch.distributed._register_builtin_comm_hook"], "torch.distributed._register_builtin_comm_hook": [], "torch.distributed._broadcast_coalesced": [], "torch.tensor": [], "torch.no_grad": [], "<builtin>.callable": [], "inspect.signature": [], "distributed.DistributedDataParallel._distributed_rank": ["torch.distributed.get_rank"], "torch.distributed.get_rank": [], "distributed.DistributedDataParallel._set_params_and_buffers_to_ignore_for_model": [], "distributed.DistributedDataParallel._get_ddp_logging_data": [], "distributed.DistributedDataParallel._set_ddp_runtime_logging_sample_rate": ["distributed.DistributedDataParallel._log_and_throw"]}
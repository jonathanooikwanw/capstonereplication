{"activation": [], "activation.Threshold.__init__": ["<builtin>.super"], "<builtin>.super": [], "activation.Threshold.forward": ["functional.threshold"], "functional.threshold": [], "activation.Threshold.extra_repr": [], "activation.ReLU.__init__": ["<builtin>.super"], "activation.ReLU.forward": ["functional.relu"], "functional.relu": [], "activation.ReLU.extra_repr": [], "activation.RReLU.__init__": ["<builtin>.super"], "activation.RReLU.forward": ["functional.rrelu"], "functional.rrelu": [], "activation.RReLU.extra_repr": [], "activation.Hardtanh.__init__": ["<builtin>.super", "warnings.warn"], "warnings.warn": [], "activation.Hardtanh.forward": ["functional.hardtanh"], "functional.hardtanh": [], "activation.Hardtanh.extra_repr": [], "activation.ReLU6.__init__": ["<builtin>.super"], "activation.ReLU6.extra_repr": [], "activation.Sigmoid.forward": ["torch.sigmoid"], "torch.sigmoid": [], "activation.Hardsigmoid.__init__": ["<builtin>.super"], "activation.Hardsigmoid.forward": ["functional.hardsigmoid"], "functional.hardsigmoid": [], "activation.Tanh.forward": ["torch.tanh"], "torch.tanh": [], "activation.SiLU.__init__": ["<builtin>.super"], "activation.SiLU.forward": ["functional.silu"], "functional.silu": [], "activation.SiLU.extra_repr": [], "activation.Mish.__init__": ["<builtin>.super"], "activation.Mish.forward": ["functional.mish"], "functional.mish": [], "activation.Mish.extra_repr": [], "activation.Hardswish.__init__": ["<builtin>.super"], "activation.Hardswish.forward": ["functional.hardswish"], "functional.hardswish": [], "activation.ELU.__init__": ["<builtin>.super"], "activation.ELU.forward": ["functional.elu"], "functional.elu": [], "activation.ELU.extra_repr": [], "activation.CELU.__init__": ["<builtin>.super"], "activation.CELU.forward": ["functional.celu"], "functional.celu": [], "activation.CELU.extra_repr": [], "activation.SELU.__init__": ["<builtin>.super"], "activation.SELU.forward": ["functional.selu"], "functional.selu": [], "activation.SELU.extra_repr": [], "activation.GLU.__init__": ["<builtin>.super"], "activation.GLU.forward": ["functional.glu"], "functional.glu": [], "activation.GLU.extra_repr": [], "activation.GELU.forward": ["functional.gelu"], "functional.gelu": [], "activation.Hardshrink.__init__": ["<builtin>.super"], "activation.Hardshrink.forward": ["functional.hardshrink"], "functional.hardshrink": [], "activation.Hardshrink.extra_repr": [], "activation.LeakyReLU.__init__": ["<builtin>.super"], "activation.LeakyReLU.forward": ["functional.leaky_relu"], "functional.leaky_relu": [], "activation.LeakyReLU.extra_repr": [], "activation.LogSigmoid.forward": ["functional.logsigmoid"], "functional.logsigmoid": [], "activation.Softplus.__init__": ["<builtin>.super"], "activation.Softplus.forward": ["functional.softplus"], "functional.softplus": [], "activation.Softplus.extra_repr": [], "activation.Softshrink.__init__": ["<builtin>.super"], "activation.Softshrink.forward": ["functional.softshrink"], "functional.softshrink": [], "activation.Softshrink.extra_repr": ["<builtin>.str"], "<builtin>.str": [], "activation.MultiheadAttention.__init__": ["torch.empty", "activation.MultiheadAttention._reset_parameters", "module.Module.register_parameter", "linear.NonDynamicallyQuantizableLinear", "<builtin>.super", "torch.nn.parameter.Parameter"], "torch.empty": [], "torch.nn.parameter.Parameter": [], "module.Module.register_parameter": [], "linear.NonDynamicallyQuantizableLinear": [], "activation.MultiheadAttention._reset_parameters": ["torch.nn.init.xavier_uniform_", "torch.nn.init.constant_", "torch.nn.init.xavier_normal_"], "torch.nn.init.xavier_uniform_": [], "torch.nn.init.constant_": [], "torch.nn.init.xavier_normal_": [], "activation.MultiheadAttention.__setstate__": ["<builtin>.super"], "activation.MultiheadAttention.forward": ["functional.multi_head_attention_forward"], "functional.multi_head_attention_forward": [], "activation.PReLU.__init__": ["<builtin>.super", "torch.empty", "torch.nn.parameter.Parameter"], "activation.PReLU.forward": ["functional.prelu"], "functional.prelu": [], "activation.PReLU.extra_repr": [], "activation.Softsign.forward": ["functional.softsign"], "functional.softsign": [], "activation.Tanhshrink.forward": ["functional.tanhshrink"], "functional.tanhshrink": [], "activation.Softmin.__init__": ["<builtin>.super"], "activation.Softmin.__setstate__": ["<builtin>.hasattr", "module.Module.__dict__.update"], "module.Module.__dict__.update": [], "<builtin>.hasattr": [], "activation.Softmin.forward": ["functional.softmin"], "functional.softmin": [], "activation.Softmin.extra_repr": [], "activation.Softmax.__init__": ["<builtin>.super"], "activation.Softmax.__setstate__": ["<builtin>.hasattr", "module.Module.__dict__.update"], "activation.Softmax.forward": ["functional.softmax"], "functional.softmax": [], "activation.Softmax.extra_repr": [], "activation.Softmax2d.forward": ["functional.softmax"], "activation.LogSoftmax.__init__": ["<builtin>.super"], "activation.LogSoftmax.__setstate__": ["<builtin>.hasattr", "module.Module.__dict__.update"], "activation.LogSoftmax.forward": ["functional.log_softmax"], "functional.log_softmax": [], "activation.LogSoftmax.extra_repr": []}
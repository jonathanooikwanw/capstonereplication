{"adamw": [], "adamw.AdamW.__init__": ["<builtin>.ValueError", "<builtin>.super", "<builtin>.dict"], "<builtin>.ValueError": [], "<builtin>.dict": [], "<builtin>.super": [], "adamw.AdamW.__setstate__": ["optimizer.Optimizer.param_groups.setdefault", "<builtin>.super"], "optimizer.Optimizer.param_groups.setdefault": [], "torch.no_grad": [], "adamw.AdamW": ["torch.no_grad"], "adamw.AdamW.step": ["torch._foreach_add", "torch._foreach_add_", "torch._foreach_addcmul_", "<builtin>.len", "torch._foreach_maximum", "torch.zeros_like", "torch._foreach_div_", "torch._foreach_sqrt", "torch._foreach_mul_", "math.sqrt", "torch.enable_grad", "torch._foreach_addcdiv_", "<builtin>.RuntimeError"], "torch.enable_grad": [], "<builtin>.RuntimeError": [], "<builtin>.len": [], "torch.zeros_like": [], "torch._foreach_mul_": [], "torch._foreach_add_": [], "torch._foreach_addcmul_": [], "torch._foreach_maximum": [], "torch._foreach_sqrt": [], "math.sqrt": [], "torch._foreach_div_": [], "torch._foreach_add": [], "torch._foreach_addcdiv_": [], "adamw.AdamW.zero_grad": ["torch._foreach_zero_", "collections.defaultdict"], "adamw.AdamW.zero_grad.<lambda1>": ["collections.defaultdict"], "collections.defaultdict": [], "torch._foreach_zero_": []}
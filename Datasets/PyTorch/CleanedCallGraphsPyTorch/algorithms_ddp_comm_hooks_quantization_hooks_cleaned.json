{"quantization_hooks": [], "quantization_hooks._quantize_per_tensor_cuda": ["torch.round", "torch.clamp"], "torch.round": [], "torch.clamp": [], "quantization_hooks._dequantize_per_tensor_cuda": [], "quantization_hooks._quantize_per_channel_cuda": ["torch.round", "torch.clamp", "<builtin>.range", "torch.zeros"], "torch.zeros": [], "<builtin>.range": [], "quantization_hooks._dequantize_per_channel_cuda": ["torch.zeros_like", "<builtin>.range"], "torch.zeros_like": [], "quantization_hooks._get_allgather_out_list": ["torch.zeros_like", "<builtin>.range"], "quantization_hooks.quantization_pertensor_hook": ["torch.FloatTensor", "quantization_hooks._get_allgather_out_list", "torch.distributed.get_rank", "torch.quantization.MinMaxObserver", "torch.distributed.all_gather"], "torch.distributed.get_rank": [], "torch.quantization.MinMaxObserver": [], "torch.FloatTensor": [], "torch.distributed.all_gather": [], "quantization_hooks.quantization_pertensor_hook.quantize_and_allgather": ["quantization_hooks._get_allgather_out_list", "torch.distributed.all_gather", "quantization_hooks._quantize_per_tensor_cuda"], "quantization_hooks.quantization_pertensor_hook.dequantize_and_aggregate": ["torch.zeros_like", "quantization_hooks._dequantize_per_tensor_cuda", "<builtin>.enumerate"], "<builtin>.enumerate": [], "quantization_hooks.quantization_perchannel_hook": ["torch.stack", "torch.nn.functional.pad", "quantization_hooks._get_allgather_out_list", "torch.distributed.get_rank", "torch.quantization.PerChannelMinMaxObserver", "<builtin>.len", "torch.distributed.all_gather"], "<builtin>.len": [], "torch.nn.functional.pad": [], "torch.quantization.PerChannelMinMaxObserver": [], "torch.stack": [], "quantization_hooks.quantization_perchannel_hook.quantize_and_allgather": ["quantization_hooks._quantize_per_channel_cuda", "torch.distributed.all_gather", "quantization_hooks._get_allgather_out_list"], "quantization_hooks.quantization_perchannel_hook.dequantize_and_aggregate": ["torch.zeros_like", "quantization_hooks._dequantize_per_channel_cuda", "<builtin>.enumerate", "torch.flatten"], "torch.flatten": []}
{"functional_adagrad": [], "functional_adagrad._FunctionalAdagrad.__init__": ["<builtin>.ValueError", "torch.jit.annotate", "torch.tensor", "<builtin>.len", "torch.full_like"], "torch.jit.annotate": [], "<builtin>.len": [], "<builtin>.ValueError": [], "torch.full_like": [], "torch.tensor": [], "functional_adagrad._FunctionalAdagrad.step": ["<builtin>.ValueError", "torch.optim._functional.adagrad", "torch.no_grad", "<builtin>.len", "<builtin>.zip"], "<builtin>.zip": [], "torch.no_grad": [], "torch.optim._functional.adagrad": []}
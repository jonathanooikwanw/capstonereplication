{"adamw": [], "adamw.AdamW.__init__": ["<builtin>.ValueError", "<builtin>.dict", "<builtin>.super"], "<builtin>.ValueError": [], "<builtin>.dict": [], "<builtin>.super": [], "adamw.AdamW.__setstate__": ["optimizer.Optimizer.param_groups.setdefault", "<builtin>.super"], "optimizer.Optimizer.param_groups.setdefault": [], "torch.no_grad": [], "adamw.AdamW": ["torch.no_grad"], "adamw.AdamW.step": ["_functional.adamw", "torch.zeros_like", "<builtin>.len", "torch.enable_grad", "<builtin>.RuntimeError"], "torch.enable_grad": [], "<builtin>.RuntimeError": [], "<builtin>.len": [], "torch.zeros_like": [], "_functional.adamw": []}